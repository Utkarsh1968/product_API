# ğŸ§  Product Q&A API (RAG-based)

This is a backend API that answers user questions about a specific set of products using a custom **Retrieval-Augmented Generation (RAG)** pipeline. It uses:

- Sentence-transformers for local embedding generation
- FAISS for in-memory vector similarity search
- Gemini or OpenAI for LLM-based answer generation
- FastAPI to expose the entire workflow as a REST API

---

## âš™ï¸ How It Works

1. **Product data** is loaded from a JSON file.
2. Descriptions and features are broken into small, meaningful text chunks.
3. Each chunk is converted to an embedding vector using a sentence transformer model (e.g., `all-MiniLM-L6-v2`).
4. The user sends a question.
5. The system generates an embedding for the question and retrieves the top-k most relevant chunks using cosine similarity with FAISS.
6. The system constructs a prompt using the retrieved chunks and the user question.
7. The prompt is passed to a Large Language Model (LLM) like Gemini 
8. The LLM responds with an answer **based only on the given context**.

---

## ğŸ—‚ï¸ Project Structure

```text
project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ server.py           # FastAPI application
â”‚   â”œâ”€â”€ loader.py         # Load and read product data
â”‚   â”œâ”€â”€ embedder.py       # Local embedding generator
â”‚   â”œâ”€â”€ vector_store.py   # In-memory FAISS vector store
â”‚   â”œâ”€â”€ retriever.py      # Retrieval of top-k relevant chunks
â”‚   â””â”€â”€ llm_client.py     # Interact with Gemini LLM
â”œâ”€â”€ data/
â”‚   â””â”€â”€ products.json     # Dataset of product info
â”œâ”€â”€ .env                  # Environment file with API keys
â”œâ”€â”€ requirements.txt      # Required Python packages
â””â”€â”€ README.md
```





## ğŸ§  Chunking Strategy

We apply a **hybrid chunking strategy**:

- Use NLTKâ€™s `sent_tokenize()` for normal-length, well-punctuated text.
- Use overlapping word-based chunking for long or poorly punctuated text (e.g., 40-word chunks with 10-word overlap).

This approach ensures:
- Meaningful, semantically rich chunks
- Better embedding quality
- Improved accuracy in retrieval and answering

---

## ğŸ“¦ Installation

### 1. Install Required Libraries

```bash
pip install -r requirements.txt

```
### 2. Add API Keys
Create a .env file in the root of your project and add your API keys:
<pre>
GEMINI_API_KEY = your_gemini_key_here
</pre>



---
## ğŸš€ Run the API Server
```bash
uvicorn App.server:app --reload
```
---
## ğŸ“¬ Example API Call

### Endpoint
```bash
POST /query
```
### Request Body
```bash
{
  "question": "What are the features of the Smartphone X200?"
}
```
### Response
```bash
{
  "answer": "Cutting-edge smartphone with a stunning display and powerful performance. Equipped with the latest processor, long battery life, and a sleek, lightweight design.",
  "source_product_ids": ["1","2"]
}
```
---
## ğŸ§  Models Used
- **Embedding model:** all-MiniLM-L6-v2 (via sentence-transformers)

- **LLM:** gemini-2.0-flash

## âœ… Design Decisions
- All embeddings are generated locally using pre-trained transformer models (no API calls for embeddings).

- No high-level frameworks like LangChain or LlamaIndex were used â€” all logic (chunking, retrieval, prompting) is implemented from scratch.

- LLM service is configurable (Gemini or OpenAI).

- Supports clean logging, modular structure, and is async-ready using FastAPI.
